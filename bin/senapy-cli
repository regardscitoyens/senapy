#!/usr/bin/env python
# -*- coding: utf-8 -*-
import csv, json, time, sys, os, glob, random
from urllib.parse import urljoin, urlparse

import requests
import click
from bs4 import BeautifulSoup
from slugify import slugify

from lawfactory_utils.urls import download, enable_requests_cache, clean_url


def _log(*args):
    print(*args, file=sys.stderr)


@click.group()
def cli():
    """
    \b
    USAGE:
        - senapy-cli parse <ID_DOSSIER_SENAT|URL_DOSSIER_SENAT|HTML_FILE>
        - senapy-cli doslegs_urls
        - cat urls | senapy-cli parse_many <output_dir>
    """
    pass


@cli.command('parse')
@click.argument('url')
@click.option('--use-cache', is_flag=True)
def cli_parse(url, use_cache):
    """
    \b
    Parse a senat dosleg given either a:
        - senat ID, ex: pjl07-488
        - URL
        - HTML file
    """
    from senapy.dosleg.parser import parse

    if use_cache:
        enable_requests_cache()

    if os.path.exists(url):
        html = open(url).read()
        data = parse(html)
    else:
        if not url.startswith('http'):
            url = "http://www.senat.fr/dossier-legislatif/%s.html" % url
        html = download(url).text
        data = parse(html, url)

    print(json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True))


@cli.command()
@click.option('--min-year', default=2009)
def doslegs_urls(min_year):
    urls = set()

    _log('finding urls in Open Data CSV...(keeping only doslegs in and after', min_year, ')')
    csv_resp = download("http://data.senat.fr/data/dosleg/dossiers-legislatifs.csv").text
    # rewrite columns titles to make it easier to work with 
    lines = [{k.lower().replace(' ' ,'_'): v for k,v in line.items()}
                for line in list(csv.DictReader(csv_resp.split('\n'), delimiter=';'))]
    link_count = 0
    for line in lines:
        year = int(line['date_initiale'].split('/')[-1])
        if year >= min_year:
            link_count += 1
            urls.add(clean_url(line['url_du_dossier']))
    _log('  =>', link_count, 'urls found in CSV')


    _log('finding urls on senat.fr')
    html = download('http://www.senat.fr/dossiers-legislatifs/textes-recents.html').text
    soup = BeautifulSoup(html, 'lxml')

    link_count = 0
    for link in soup.select('a'):
        href = link.attrs.get('href', '')
        if '/dossier-legislatif/' in href:
            link_count += 1
            url = urljoin('http://www.senat.fr/', href)
            urls.add(clean_url(url))
    _log('  =>', link_count, 'doslegs found on website')

    for url in sorted(urls):
        print(url)



@cli.command()
@click.argument('output_dir')
@click.option('--overwrite', is_flag=True)
@click.option('--disable-cache', is_flag=True)
def parse_many(output_dir, overwrite, disable_cache):
    from senapy.dosleg.parser import parse

    if not disable_cache:
        enable_requests_cache()

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for url in sys.stdin:
        url = url.strip()

        senat_id = url.split('/')[-1].replace('.html', '')

        filepath = os.path.join(output_dir, senat_id)
        if not overwrite and os.path.exists(filepath):
            continue

        _log(' -- ', url)
        html = download(url).text
        parsed = parse(html, url)
        json.dump(parsed, open(filepath, 'w'), ensure_ascii=False, indent=2, sort_keys=True)
    _log('parsing finished')


if __name__ == '__main__':
    cli()

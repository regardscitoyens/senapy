#!/usr/bin/env python
# -*- coding: utf-8 -*-
import csv, json, time, sys, os, glob, random
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from slugify import slugify


if len(sys.argv) < 2:
    print("""USAGE:
    - senapy-cli parse <ID_DOSSIER_SENAT|URL_DOSSIER_SENAT|HTML_FILE>
    - senapy-cli download_recent <DIR>
    - senapy-cli download_from_csv <DIR>
    - senapy-cli parse_directory <glob_pattern> <PARSED_DIR>
    """)
    sys.exit()


if sys.argv[1] == 'parse':
    from senapy.dosleg.parser import parse

    if len(sys.argv) < 3:
        print('USAGE: senapy-cli parse c')
        sys.exit()

    url = sys.argv[2]
    if os.path.exists(url):
        html = open(url).read()
        data = parse(html)
    else:
        if not url.startswith('http'):
            url = "http://www.senat.fr/dossier-legislatif/%s.html" % url
        html = requests.get(url).text
        data = parse(html, url)

    print(json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True))

elif sys.argv[1] == 'download_recent':
    dest = sys.argv[2]
    print('saving to', dest)

    if not os.path.exists(dest):
        os.makedirs(dest)

    html = requests.get('http://www.senat.fr/dossiers-legislatifs/textes-recents.html').text
    soup = BeautifulSoup(html, 'html5lib')

    link_count = 0
    for link in soup.select('#main .box.box-type-02 .box-inner.gradient-01 a'):
        href = link.attrs['href']
        if '/dossier-legislatif/' in href:
            link_count += 1
            print(link.attrs['href'])
            print(link.text.strip())
            print()

            dest_file = os.path.join(dest, slugify(link.attrs['href']))
            if os.path.exists(dest_file):
                continue

            resp = requests.get('http://www.senat.fr' + link.attrs['href'])
            url = urljoin('http://www.senat.fr/', href)
            open(dest_file, 'w').write(resp.text
                + ('\n<!-- URL_SENAT=%s -->' % url))
    print(link_count, 'doslegs found')

elif sys.argv[1] == 'download_from_csv':
    csv_resp = requests.get("http://data.senat.fr/data/dosleg/dossiers-legislatifs.csv")

    lines = [{k.lower().replace(' ' ,'_'): v for k,v in line.items()}
                for line in list(csv.DictReader(csv_resp.text.split('\n'), delimiter=';'))]

    dest = sys.argv[2]

    if not os.path.exists(dest):
        os.makedirs(dest)

    lines.sort(key=lambda line:int(line['date_initiale'].split('/')[-1])) # sort by year

    link_count = 0
    for line in lines:
        year = int(line['date_initiale'].split('/')[-1])
        if year >= 2008:
            link_count += 1
            print(year, line['titre'])

            dest_file = os.path.join(dest, slugify(line['url_du_dossier']))
            if os.path.exists(dest_file):
                continue

            resp = requests.get(line['url_du_dossier'])
            open(dest_file, 'w').write(resp.text
                + ('\n<!-- URL_SENAT=%s -->' % line['url_du_dossier']))
            time.sleep(0.2)
    print(link_count, 'doslegs found')

elif sys.argv[1] == 'parse_directory':
    from senapy.dosleg.parser import parse

    files = glob.glob(sys.argv[2])
    random.shuffle(files)

    print(len(files), 'files to parse')
    for file in files:
        html = open(file).read()
        filepath = os.path.join(sys.argv[3], file.split('/')[-1])
        filepath = filepath.replace('http-www-senat-fr-', '')

        if os.path.exists(filepath):
            continue

        print('parsing', file)
        data = parse(html)
        json.dump(data, open(filepath, 'w'), ensure_ascii=False, indent=2, sort_keys=True)

else:
    print('INVALID COMMAND')
